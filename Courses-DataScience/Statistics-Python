{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMMpN4r8GKRDEncf0Q7ulxo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction to Statistics"],"metadata":{"id":"99bwLIRvvKY9"}},{"cell_type":"markdown","source":["\n","## Type of Data\n","\n","- Numeric (Quantitative)\n"," - Continous (Measured)\n"," - Discrete (Counted)\n","\n","- Categorical (Qualitative)\n","  - Nominal (Unordered)\n","  - Ordinal (Ordered)\n","   \n"],"metadata":{"id":"S6qrDWLCvSGj"}},{"cell_type":"markdown","source":["## Basic Statistics"],"metadata":{"id":"b2O_SiJuvSny"}},{"cell_type":"markdown","source":["### Mean and Median"],"metadata":{"id":"ETJ2NuRe13cv"}},{"cell_type":"code","source":["# Import numpy with alias np\n","import numpy as np\n","\n","# Filter for Belgium\n","be_consumption = food_consumption[food_consumption['country'] == \"Belgium\"]\n","\n","# Filter for USA\n","usa_consumption = food_consumption[food_consumption['country'] == \"USA\"]\n","\n","# Calculate mean and median consumption in Belgium\n","print(be_consumption['consumption'].agg(np.mean))\n","print(be_consumption['consumption'].agg(np.median))\n","\n","# Calculate mean and median consumption in USA\n","print(usa_consumption['consumption'].agg(np.mean))\n","print(usa_consumption['consumption'].agg(np.median))"],"metadata":{"id":"XQlD_9gPvPXB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Spread\n","\n"],"metadata":{"id":"vDBYIrH9e6F8"}},{"cell_type":"markdown","source":["- Variance\n","- Standar Deviation\n","- Quantiles\n","- Quartiles\n","- BoxPlots\n","- IQR (interquartile range)"],"metadata":{"id":"OVmiPALDguzJ"}},{"cell_type":"markdown","source":["### Quartiles and Quintiles"],"metadata":{"id":"BcoUscLpi0NG"}},{"cell_type":"code","source":["# Calculate the deciles of co2_emission\n","print(np.quantile(food_consumption[\"co2_emission\"], np.linspace(0,1,11))) # Eleven quantiles, split the data into 10 pieces... 0 for start, 1 for end, 11 for total pieces\n","\n","# Calculate the deciles of co2_emission\n","print(np.quantile(food_consumption[\"co2_emission\"], np.linspace(0,1,5))) # A simple cuartile"],"metadata":{"id":"AQz1RRkfe7eS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variance and Std"],"metadata":{"id":"ye-U-Z2zk-Eg"}},{"cell_type":"code","source":["# Print variance and sd of co2_emission for each food_category\n","print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n","\n","# Import matplotlib.pyplot with alias plt\n","import matplotlib.pyplot as plt\n","\n","# Create histogram of co2_emission for food_category 'beef'\n","food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n","# Show plot\n","plt.show()\n","\n","# Create histogram of co2_emission for food_category 'eggs'\n","food_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()\n","# Show plot\n","plt.show()"],"metadata":{"id":"Uj2f8_sklAqq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["IQR, Outliers, Cuttsoff"],"metadata":{"id":"ByxdQFJVnNKs"}},{"cell_type":"code","source":["# Calculate total co2_emission per country: emissions_by_country\n","emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n","\n","# Compute the first and third quantiles and IQR of emissions_by_country\n","q1 = np.quantile(emissions_by_country, 0.25)\n","q3 = np.quantile(emissions_by_country, 0.75)\n","iqr = q3 - q1\n","\n","# Calculate the lower and upper cutoffs for outliers\n","lower = q1 - 1.5 * iqr\n","upper = q3 + 1.5 * iqr\n","\n","# Subset emissions_by_country to find outliers\n","outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)] #Find the upper values and the lower ones\n","print(outliers)"],"metadata":{"id":"AlPscJTVnTlE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Probabilties"],"metadata":{"id":"JnmsDEy6oK4I"}},{"cell_type":"markdown","source":["###Calculate Probabilties"],"metadata":{"id":"GFyDEn7Tunft"}},{"cell_type":"code","source":["# Count the total deals for each product\n","counts = amir_deals['product'].value_counts()\n","\n","# Calculate probability of picking a deal with each product\n","probs = counts / amir_deals.shape[0] #this shape[0], allows us to obtain the total numer of deals\n","print(probs)"],"metadata":{"id":"n5-9LU06uppW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sample : Take any rows random and choose if you want with replacement or with out"],"metadata":{"id":"rSoLz411rOmz"}},{"cell_type":"code","source":["# Set random seed\n","np.random.seed(22)\n","\n","# Sample 5 deals without replacement\n","sample_without_replacement = amir_deals.sample(5,replace = False)\n","print(sample_without_replacement)"],"metadata":{"id":"YXmuBk87rOKz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Discrete Probability\n"],"metadata":{"id":"T6SB7QNuEgRK"}},{"cell_type":"code","source":["# Create probability distribution\n","size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n","# Reset index and rename columns\n","size_dist = size_dist.reset_index()\n","size_dist.columns = ['group_size', 'prob']\n","\n","# Expected value\n","expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n","\n","# Subset groups of size 4 or more\n","groups_4_or_more = size_dist[size_dist['group_size'] >= 4]\n","\n","# Sum the probabilities of groups_4_or_more\n","prob_4_or_more = np.sum(groups_4_or_more['prob'])\n","print(prob_4_or_more)"],"metadata":{"id":"28dGDsG2Eis0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Continous Distribution"],"metadata":{"id":"yiDOD97-PXDb"}},{"cell_type":"markdown","source":["### Uniform Probability"],"metadata":{"id":"-TSvXk-QPatJ"}},{"cell_type":"code","source":["# Min and max wait times for back-up that happens every 30 min\n","min_time = 0\n","max_time = 30\n","\n","# Import uniform from scipy.stats\n","from scipy.stats import uniform\n","\n","# Calculate probability of waiting 10-20 mins\n","prob_between_10_and_20 = prob_less_than_5 = uniform.cdf(20, min_time, max_time) - uniform.cdf(10, min_time, max_time)\n","print(prob_between_10_and_20)"],"metadata":{"id":"9yyx65JIPZx2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Simulating Random distributions"],"metadata":{"id":"UAnmF9zASG0x"}},{"cell_type":"code","source":["# Set random seed to 334\n","np.random.seed(334)\n","\n","# Import uniform\n","from scipy.stats import uniform\n","\n","# Generate 1000 wait times between 0 and 30 mins\n","wait_times = uniform.rvs(0, 30, size=1000)\n","\n","# Create a histogram of simulated times and show plot\n","plt.hist(wait_times, bins = 1000)\n","plt.show()"],"metadata":{"id":"uwj8A75qSLe3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Binominal Distribution"],"metadata":{"id":"QbxQZkUMSR4H"}},{"cell_type":"markdown","source":["- binom.pmf() probabilidad de que de la misma cara de la moneda\n","- binom.cdf() probabilidad de que de la misma cara o menos\n","-"],"metadata":{"id":"nAdZ-4zETSUZ"}},{"cell_type":"markdown","source":["### Distribition"],"metadata":{"id":"4fF0CWM-V9pn"}},{"cell_type":"code","source":["# Import binom from scipy.stats\n","from scipy.stats import binom\n","\n","# Set random seed to 10\n","np.random.seed(10)\n","\n","# Simulate 52 weeks of 3 deals\n","deals = binom.rvs(3, 0.3, size=52)\n","\n","# Print mean deals won per week\n","print(deals.mean())"],"metadata":{"id":"MirkTZdTSUS8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Probability that excat N of try"],"metadata":{"id":"X08Qq8LCWHw0"}},{"cell_type":"code","source":["# Probability of closing 3 out of 3 deals\n","prob_3 = binom.pmf(3, 3, 0.3,)\n","\n","print(prob_3)"],"metadata":{"id":"bb0vh7DSWNc0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Probability that the N or fewer or greater"],"metadata":{"id":"K09KGL8BWvul"}},{"cell_type":"code","source":["# Probability of closing <= 1 deal out of 3 deals\n","prob_less_than_or_equal_1 = binom.cdf(1, 3, 0.3)\n","\n","print(prob_less_than_or_equal_1)\n","\n","# Probability of closing > 1 deal out of 3 deals\n","prob_greater_than_1 = 1 - binom.cdf(1,3,0.3)\n","\n","\n","print(prob_greater_than_1)"],"metadata":{"id":"spU4JtvDW00K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Normal Distribution"],"metadata":{"id":"ZbDbI58DXuZX"}},{"cell_type":"markdown","source":["- Symetrical\n","- Area beneath the curve = 1\n","- Curve or probability never hits 0\n","- it's describe by the mean and std\n","\n"],"metadata":{"id":"UD8yDPJcV8ss"}},{"cell_type":"code","source":["# Probability of deal between 3000 and 7000\n","prob_3000_to_7000 = norm.cdf(7000, 5000, 2000) - norm.cdf(3000, 5000, 2000)\n","\n","print(prob_3000_to_7000)"],"metadata":{"id":"QNjTV3_7X3rB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate amount that 25% of deals will be less than\n","pct_25 = norm.ppf(0.25, 5000, 2000)\n","\n","print(pct_25)"],"metadata":{"id":"eYyj0L79a4FN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Simulating sales"],"metadata":{"id":"fifENVqobsiw"}},{"cell_type":"code","source":["# Calculate new average amount\n","new_mean = 5000 + 5000*0.2\n","\n","# Calculate new standard deviation\n","new_sd = 2000 + 2000 * 0.3\n","# Simulate 36 new sales\n","new_sales = norm.rvs(new_mean, new_sd, size = 36)\n","# Create histogram and show\n","plt.hist(new_sales, bins = 36)\n","plt.show()"],"metadata":{"id":"TxRyi8h2buuz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Central Limit Theorem"],"metadata":{"id":"qKXu6BgTzgxS"}},{"cell_type":"code","source":["# Set seed to 104\n","np.random.seed(104)\n","\n","sample_means = []\n","# Loop 100 times\n","for i in range(100):\n","  # Take sample of 20 num_users\n","  samp_20 = amir_deals['num_users'].sample(20, replace=True)\n","  # Calculate mean of samp_20\n","  samp_20_mean = np.mean(samp_20)\n","  # Append samp_20_mean to sample_means\n","  sample_means.append(samp_20_mean)\n","\n","# Convert to Series and plot histogram\n","sample_means_series = pd.Series(sample_means)\n","sample_means_series.hist()\n","# Show plot\n","plt.show()"],"metadata":{"id":"ziXITUeLzjEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set seed to 321\n","np.random.seed(321)\n","\n","sample_means = []\n","# Loop 30 times to take 30 means\n","for i in range(30):\n","  # Take sample of size 20 from num_users col of all_deals with replacement\n","  cur_sample = all_deals['num_users'].sample(20, replace=True)\n","  # Take mean of cur_sample\n","  cur_mean = np.mean(cur_sample)\n","  # Append cur_mean to sample_means\n","  sample_means.append(cur_mean)\n","\n","# Print mean of sample_means\n","print(np.mean(sample_means))\n","\n","# Print mean of num_users in amir_deals\n","print(np.mean(amir_deals['num_users']))"],"metadata":{"id":"KbACWwdRzyjY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Poisson Distribution"],"metadata":{"id":"SBzHg164z5o3"}},{"cell_type":"markdown","source":["- Lambda : average dnumber of event per time interval\n","- Example: Average number of adoptions per week = 8\n","- Discrete distribution"],"metadata":{"id":"oqnr5Sia0ngp"}},{"cell_type":"code","source":["# Import poisson from scipy.stats\n","from scipy.stats import poisson\n","\n","# Probability of 2 or fewer responses\n","prob_2_or_less = poisson.cdf(2, 4)\n","\n","print(prob_2_or_less)"],"metadata":{"id":"KyI19LdNz7xU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exponential Distribution"],"metadata":{"id":"VhMyXl4W4NdN"}},{"cell_type":"code","source":["# Import expon from scipy.stats\n","from scipy.stats import expon\n","\n","# Print probability response takes 3-4 hours\n","print(expon.cdf(4, scale=2.5) - expon.cdf(3, scale=2.5))"],"metadata":{"id":"kw5tq3PJ4PL5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Correlation"],"metadata":{"id":"sYg8d8514b7I"}},{"cell_type":"code","source":["# Create a scatterplot of happiness_score vs. life_exp and show\n","sns.scatterplot(x='life_exp', y='happiness_score', data=world_happiness)\n","\n","# Show plot\n","plt.show()"],"metadata":{"id":"Mcnt4ji_5gER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create scatterplot of happiness_score vs life_exp with trendline\n","sns.lmplot(x='life_exp', y='happiness_score', data=world_happiness, ci=None)\n","\n","# Show plot\n","plt.show()\n","\n","# Correlation between life_exp and happiness_score\n","cor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])  #This calculate the correlation\n","\n","print(cor)"],"metadata":{"id":"J_Ty9wTR4dB5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Non linear Relationships"],"metadata":{"id":"_cHS8MfS5w6c"}},{"cell_type":"markdown","source":["- Log Transformation log(x)\n","- Square Trans. sqrt(x)\n","- Reciprocal Trans. 1/x\n"],"metadata":{"id":"eQ0ucICC6CDv"}},{"cell_type":"code","source":["# Create log_gdp_per_cap column\n","world_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap']) # here we apply a log funtion in order to linear the data\n","\n","# Scatterplot of happiness_score vs. log_gdp_per_cap\n","sns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness)\n","plt.show()\n","\n","# Calculate correlation\n","cor = world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\n","print(cor)"],"metadata":{"id":"sEjm76_c5zPY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Desing of Experiments"],"metadata":{"id":"qPhgNnqU86iq"}},{"cell_type":"markdown","source":["- Controlled Experiments\n","- Observeational Studies\n","- Longitudinal study vs cross-sectional studies"],"metadata":{"id":"i6OzJi7W8_6g"}},{"cell_type":"code","source":[],"metadata":{"id":"ojjEQnAC89CX"},"execution_count":null,"outputs":[]}]}